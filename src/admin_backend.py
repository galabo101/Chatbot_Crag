import os
import sqlite3
import shutil
import base64
import time
from collections import Counter
import pandas as pd
import json
from datetime import datetime
import fitz 
from docx import Document
from groq import Groq
from src.embedding.indexer import QdrantIndexer
from src.security.security import SecurityManager 
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from src.database import add_document, delete_document_record, get_all_documents 
from dotenv import load_dotenv
load_dotenv()

# --- C·∫§U H√åNH ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
MODEL_ID = "meta-llama/llama-4-scout-17b-16e-instruct"
DB_FILE = "chat_history.db"

# ƒê∆∞·ªùng d·∫´n file stopwords
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
STOPWORDS_PATH = os.path.join(BASE_DIR, "data", "vietnamese-stopwords.txt")

security_manager = SecurityManager()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", "!", "?", ";", " ", ""],
    length_function=len
)

def load_vietnamese_stopwords():
    default_stopwords = {
        "kh√¥ng", "ƒë∆∞·ª£c", "nh·ªØng", "trong", "nh∆∞ng", "c≈©ng", "n√†y", 
        "ƒëang", "v·ªõi", "theo", "r·∫±ng", "vi·ªác", "ng∆∞·ªùi", "ch√∫ng", "c·ªßa", "v√†", "l√†"
    }    
    if os.path.exists(STOPWORDS_PATH):
        try:
            with open(STOPWORDS_PATH, "r", encoding="utf-8") as f:
                return {line.strip() for line in f if line.strip()}
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói ƒë·ªçc file stopwords: {e}. D√πng m·∫∑c ƒë·ªãnh.")
            return default_stopwords
    return default_stopwords

def get_chat_stats():  
    try:  #L·∫•y th·ªëng k√™ t·ªïng quan
        conn = sqlite3.connect(DB_FILE)
        df_convs = pd.read_sql("SELECT COUNT(*) as cnt FROM conversations", conn)
        total_convs = df_convs.iloc[0]['cnt'] if not df_convs.empty else 0
        
        df_msgs = pd.read_sql("SELECT COUNT(*) as cnt FROM messages", conn)
        total_msgs = df_msgs.iloc[0]['cnt'] if not df_msgs.empty else 0
        
        recent = pd.read_sql("""
            SELECT content, created_at FROM messages 
            WHERE role='user' ORDER BY created_at DESC LIMIT 20
        """, conn)
        conn.close()
        
        return {
            'total_conversations': total_convs,
            'total_messages': total_msgs,
            'recent_questions': recent
        }
    except Exception as e:
        print(f"Stats Error: {e}")
        return {
            'total_conversations': 0,
            'total_messages': 0,
            'recent_questions': pd.DataFrame()
        }

def get_top_keywords():
    try:  #Ph√¢n t√≠ch t·ª´ kh√≥a n·ªïi b·∫≠t
        stopwords = load_vietnamese_stopwords()
        
        conn = sqlite3.connect(DB_FILE)
        msgs = pd.read_sql("SELECT content FROM messages WHERE role='user'", conn)
        conn.close()
        
        if msgs.empty: return []

        all_keywords = []
        for content in msgs['content']:
            if not content: continue
            words = str(content).lower().split()
            filtered = [w for w in words if len(w) > 2 and w not in stopwords]
            all_keywords.extend(filtered)
        
        return Counter(all_keywords).most_common(10)
    except Exception as e:
        print(f"Keyword Error: {e}")
        return []

class GroqParser:
    def __init__(self):
        self.client = Groq(api_key=GROQ_API_KEY)

    def encode_image(self, image_bytes):
        return base64.b64encode(image_bytes).decode('utf-8')

    def parse_page_to_markdown(self, image_bytes, page_num, retry_count=0):        
        base64_image = self.encode_image(image_bytes)
        
        prompt = """B·∫°n l√† m·ªôt c√¥ng c·ª• chuy·ªÉn ƒë·ªïi t√†i li·ªáu ch√≠nh x√°c (OCR & Layout Analysis).
Nhi·ªám v·ª•: Chuy·ªÉn ƒë·ªïi to√†n b·ªô n·ªôi dung trong h√¨nh ·∫£nh trang t√†i li·ªáu n√†y th√†nh ƒë·ªãnh d·∫°ng MARKDOWN.

Y√™u c·∫ßu b·∫Øt bu·ªôc:
1. GI·ªÆ NGUY√äN c·∫•u tr√∫c: Ti√™u ƒë·ªÅ d√πng #, B·∫£ng bi·ªÉu d√πng Markdown Table (|...|), Danh s√°ch d√πng (-).
2. TR√çCH XU·∫§T CH√çNH X√ÅC: Kh√¥ng t√≥m t·∫Øt, kh√¥ng b·ªè s√≥t ch·ªØ, gi·ªØ nguy√™n ti·∫øng Vi·ªát.
3. KH√îNG th√™m l·ªùi d·∫´n: Kh√¥ng tr·∫£ l·ªùi "ƒê√¢y l√† n·ªôi dung...", ch·ªâ tr·∫£ v·ªÅ n·ªôi dung Markdown thu·∫ßn t√∫y.
4. N·∫øu c√≥ h√¨nh ·∫£nh minh h·ªça (kh√¥ng ph·∫£i vƒÉn b·∫£n), h√£y m√¥ t·∫£ n√≥ trong ngo·∫∑c vu√¥ng: [H√¨nh ·∫£nh: m√¥ t·∫£...].
"""
        try:
            response = self.client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
                        ],
                    }
                ],
                model=MODEL_ID,
                temperature=0.0, 
                max_tokens=4096, 
            )
            print(f"   ‚úÖ ƒê√£ parse xong trang {page_num} (Model: {MODEL_ID})")
            return response.choices[0].message.content
        except Exception as e:
            if retry_count < 2:
                print(f"   üîÑ L·ªói m·∫°ng/Rate Limit trang {page_num}. Th·ª≠ l·∫°i l·∫ßn {retry_count + 1}...")
                time.sleep(5)
                return self.parse_page_to_markdown(image_bytes, page_num, retry_count + 1)
            print(f"   ‚ùå B·ªè qua trang {page_num}: {str(e)}")
            return ""


def process_uploaded_file(uploaded_file, client=None, model=None):
    
    # 1. SECURITY CHECK
    is_valid, error_msg = security_manager.validate_file(uploaded_file)
    if not is_valid:
        raise ValueError(f"B·∫£o m·∫≠t t·ª´ ch·ªëi: {error_msg}")

    save_dir = "./data/uploaded_temp"
    try:
        os.makedirs(save_dir, exist_ok=True)
        safe_filename = security_manager.get_safe_filename(uploaded_file.name)
        file_path = os.path.join(save_dir, safe_filename)        
        # ƒê·ªçc d·ªØ li·ªáu v√†o b·ªô nh·ªõ ƒë·ªám
        file_bytes = uploaded_file.getbuffer()
        with open(file_path, "wb") as f:
            f.write(file_bytes)
            
        parser = GroqParser()
        full_markdown_text = ""
        original_display_name = uploaded_file.name
        doc_type = "text_plain" # M·∫∑c ƒë·ªãnh        

        if original_display_name.lower().endswith('.pdf'):
            doc_type = "text_markdown"
            try:
                doc = fitz.open(file_path)
                total_pages = len(doc)
                print(f"üìÑ Qu√©t PDF {total_pages} trang...")
                for page_num in range(total_pages):
                    page = doc.load_page(page_num)
                    pix = page.get_pixmap(dpi=150) 
                    img_bytes = pix.tobytes("png")
                    page_markdown = parser.parse_page_to_markdown(img_bytes, f"Trang {page_num+1}")
                    full_markdown_text += f"\n\n--- Trang {page_num + 1} ---\n\n" + (page_markdown or "")
                    time.sleep(2) 
            except Exception as e: print(f"L·ªói PDF: {e}")

        elif original_display_name.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):
            doc_type = "image_description"
            try:
                print(f"üñºÔ∏è ƒêang ph√¢n t√≠ch h√¨nh ·∫£nh: {original_display_name}...")
                # G·ª≠i th·∫≥ng bytes c·ªßa ·∫£nh l√™n Groq
                image_desc = parser.parse_page_to_markdown(bytes(file_bytes), "Image File")
                full_markdown_text = f"**[M√¥ t·∫£ h√¨nh ·∫£nh: {original_display_name}]**\n\n{image_desc}"
            except Exception as e: print(f"L·ªói ·∫¢nh: {e}")

        elif original_display_name.lower().endswith('.xlsx'):
            doc_type = "table_markdown"
            try:
                df = pd.read_excel(file_path)
                full_markdown_text = df.to_markdown(index=False)
                print(f"üìä ƒê√£ ƒë·ªçc Excel: {len(df)} d√≤ng")
            except Exception as e: print(f"L·ªói Excel: {e}")

        elif original_display_name.lower().endswith('.json'):
            doc_type = "json_text"
            try:
                # ƒê·ªçc JSON v√† chuy·ªÉn th√†nh chu·ªói ƒë·∫πp
                json_content = json.load(open(file_path, 'r', encoding='utf-8'))
                full_markdown_text = json.dumps(json_content, ensure_ascii=False, indent=2)
                print(f"DATA ƒê√£ ƒë·ªçc JSON")
            except Exception as e: print(f"L·ªói JSON: {e}")                
   
        elif original_display_name.endswith('.docx'):
            try:
                doc = Document(file_path)
                full_markdown_text = "\n".join([para.text for para in doc.paragraphs])
            except: pass            
        else: 
            try: full_markdown_text = str(uploaded_file.read(), "utf-8")
            except: pass

        # CHUNKING & INDEXING
        chunks_data = []
        if full_markdown_text and full_markdown_text.strip():
            text_chunks = text_splitter.split_text(full_markdown_text)
            
            for content in text_chunks:
                chunks_data.append({
                    "content": content,
                    "type": doc_type,
                    "source": "admin_upload_multimodal"
                })

        if chunks_data:
            indexer = QdrantIndexer(
                qdrant_path="./qdrant_data",
                collection_name="bdu_chunks_gemma",
                embedding_model="google/embeddinggemma-300m",
                client=client,
                model=model
            )
            
            temp_jsonl = os.path.join(save_dir, "temp_chunks.jsonl")
            with open(temp_jsonl, "w", encoding="utf-8") as f:
                for i, item in enumerate(chunks_data):
                    payload = {
                        "chunk_id": f"upload_{datetime.now().timestamp()}_{i}",
                        "content": item["content"], 
                        "url": "T√†i li·ªáu Admin Upload",
                        "title": original_display_name, 
                        "type": item["type"],
                        "full_content": item["content"] 
                    }
                    f.write(json.dumps(payload, ensure_ascii=False) + "\n")
            indexer.index_jsonl(temp_jsonl)
            
            # L∆∞u v√†o SQLite ƒë·ªÉ qu·∫£n l√Ω nhanh
            add_document(original_display_name, len(chunks_data))
            
        return len(chunks_data)

    except Exception as e:
        print(f"‚ùå Critical Error: {e}")
        raise e 
    finally:       
        if os.path.exists(save_dir):
            try: shutil.rmtree(save_dir)
            except: pass

def get_all_files(client=None):
    # ∆Øu ti√™n l·∫•y t·ª´ SQLite (nhanh & c√≥ sort)
    docs = get_all_documents()
    if docs:
        return docs
    
    # Fallback: N·∫øu SQLite r·ªóng (ch∆∞a sync), l·∫•y t·ª´ Qdrant (ch·∫≠m)
    # L∆∞u √Ω: Return format kh√°c nhau, UI c·∫ßn x·ª≠ l√Ω
    try:
        indexer = QdrantIndexer(
            qdrant_path="./qdrant_data",
            collection_name="bdu_chunks_gemma",
            embedding_model="google/embeddinggemma-300m",
            client=client
        )
        titles = indexer.get_all_titles()
        # Mock struct ƒë·ªÉ t∆∞∆°ng th√≠ch UI
        return [{"filename": t, "upload_time": "N/A", "num_chunks": "?"} for t in titles]
    except Exception as e:
        print(f"Error getting files: {e}")
        return []

def delete_doc(file_name, client=None):
    try:
        indexer = QdrantIndexer(
            qdrant_path="./qdrant_data",
            collection_name="bdu_chunks_gemma",
            embedding_model="google/embeddinggemma-300m",
            client=client
        )
        # 1. X√≥a trong Vector DB
        indexer.delete_by_title(file_name)
        
        # 2. X√≥a trong SQLite
        delete_document_record(file_name)
        
        return True
    except Exception as e:
        print(f"Error deleting file: {e}")
        raise e

def sync_documents_from_qdrant(client=None):
    """Qu√©t to√†n b·ªô Qdrant ƒë·ªÉ c·∫≠p nh·∫≠t v√†o SQLite"""
    try:
        indexer = QdrantIndexer(
            qdrant_path="./qdrant_data",
            collection_name="bdu_chunks_gemma",
            embedding_model="google/embeddinggemma-300m",
            client=client
        )
        titles = indexer.get_all_titles()
        count = 0
        for t in titles:
            add_document(t, 0) # Kh√¥ng bi·∫øt s·ªë chunks ch√≠nh x√°c, ƒë·ªÉ 0
            count += 1
        return count
    except Exception as e:
        print(f"Sync error: {e}")
        return 0

def get_file_details(file_name, client=None):
    try:
        indexer = QdrantIndexer(
            qdrant_path="./qdrant_data",
            collection_name="bdu_chunks_gemma",
            embedding_model="google/embeddinggemma-300m",
            client=client
        )
        return indexer.get_file_chunks(file_name)
    except Exception as e:
        print(f"Error getting file details: {e}")
        return []